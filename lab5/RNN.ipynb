{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG67OQHja5_a"
      },
      "source": [
        "# Загрузка датасета "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAvbZjO9a-qi"
      },
      "source": [
        "Загрузим датасет №5 с Wikibooks ([датасет](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SGnM0j8ea4At"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyJ3tVkv4-ti",
        "outputId": "7579d303-e4f9-4597-fcd2-e5124a59dd03"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGEKGLeMxsth",
        "outputId": "f5cfcdb5-5068-48ba-d760-997c87cefe2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.13)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.65.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.26.15)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4)\n",
            "rm: cannot remove '/root/.kaggle': No such file or directory\n",
            "Downloading wikibooks-dataset.zip to /content\n",
            " 99% 1.81G/1.82G [00:17<00:00, 116MB/s]\n",
            "100% 1.82G/1.82G [00:17<00:00, 114MB/s]\n",
            "Archive:  wikibooks-dataset.zip\n",
            "  inflating: wikibooks.sqlite        \n",
            "rm: cannot remove '*.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!rm -r ~/.kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!echo '{\"username\":\"midlow\",\"key\":\"19e4a7b3c26e4d040a5179c6b36318cd\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dhruvildave/wikibooks-dataset\n",
        "!unzip wikibooks-dataset.zip\n",
        "!rm wikibooks-dataset.zip\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeiBSBDysy_k"
      },
      "source": [
        "Возьмём из базы англоязычные книги."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebK9y3r7zQck",
        "outputId": "9b36c5e4-428f-4aa2-b666-2ce9f8bd8a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Front Page: Radiation Oncology | RTOG Trials | Randomized Trials\\n\\n\\n\\n\\nNon-Hodgkin lymphoma: Main Page  | Randomized\\nOverview: Overview  | \\nFollicular |\\nDiffuse large B-cell |\\nMALT |\\nNodal marginal zone |\\nMantle cell |\\nCLL/SLL |\\nLymphoblastic |\\nBurkitt |\\nNK/T cell |\\nAnaplastic large cell |\\nPrimary CNS Lymphoma\\nTreatment:\\nAggressive |\\nSpecific sites |\\nRadioimmunotherapy\\n\\n\\n\\nChronic Lymphocytic Leukemia and Small Lymphocytic Lymphoma (CLL/SLL)\\n\\n\\nContents\\n\\n1 Overview\\n2 Staging\\n3 Classification\\n4 Richter\\'s transformation\\n5 Radiation Therapy\\n6 Reviews\\n\\n\\n\\nOverview[edit\\xa0| edit source]\\nCLL is the most common leukemia among adults in Western world\\nIt is characterized by accumulation of mature B-cells\\nCLL molecular phenotype: CD5+, CD23+, surface Ig weak, CD79b weak/absent, FMC7 neg.\\nDiagnosis: lymphocytosis (often >5 x 10^9 / L, but not an absolute cutoff)\\nRisk factors are male sex, advanced age, white race, and family history of CLL or lymphoproliferative disorders\\nCause and pathogenesis are largely unknown\\nPeripheral blood of some healthy adults shows circulating B-cell clones with surface phenotypes similar to CLL\\nMonocloncal B-cell lymphomatosis (MBL) indicates presence of such B-cells in blood at <5000 per cubic millimeter\\nPrevalence of MBL may be 3-5% in general population over 50 years of age\\nIt appears that these circulating B-cell clones may be present in blood of for years prior to development of CLL, and that the light chain re-arrangement is the same\\nThus, it may be reasonable to consider MBL a precursor state to CLL, in a similar fashion as MGUS is a precursor state to multiple myeloma\\nCLL and SLL are histologically and immunophenotypically identical\\nBy definition, CLL has more marked peripheral blood involvement\\nCLL: absolute lymphocyte count >=5 x109/L\\nSLL: absolute lymphocyte count <5 x109/L\\nClinical course varies widely, but frequently characterized by indolent behavior\\nTreatment commonly deferred while patients asymptomatic\\nNo consensus on best treatment, but nucleoside analogues and Rituxan have led to improved outcomes\\nStaging[edit\\xa0| edit source]\\nRai Staging\\n\\n\\n\\n\\nRai Stage\\nCriteria\\nSurvival (yr)\\n\\n\\n0\\nlymphocytosis only. no other abnormality\\n> 13\\n\\n\\nI\\nlymphocytosis and lymph node enlargement. no other abnormality\\n8\\n\\n\\nII\\nlymphocytosis and spleen or liver enlargement (w/ or w/o lymph node enlargement). no other abnormality\\n5\\n\\n\\nIII\\nlymphocytosis and anemia (Hgb < 11 g/dL); w/ or w/o spleen / liver / lymph node enlargement. no platelet abnormality\\n2\\n\\n\\nIV\\nlymphocytosis and thrombocytopenia (plts < 100,000 /µL)\\n1\\n\\nBinet Staging\\n\\n\\nBinet Stage\\nCriteria\\nSurvival (yr)\\n\\n\\nA\\n<3 lymph node areas; no anemia or thrombocytopenia\\n12\\n\\n\\nB\\n3 or more lymph node areas; no anemia or thrombocytopenia\\n5\\n\\n\\nC\\nAnemia (Hgb < 11) or thrombocytopenia (< 100,000 /µL)\\n2\\n\\nClassification[edit\\xa0| edit source]\\nMD Anderson; 2007 (1985-2005) PMID 17925562 -- \"Assessment of chronic lymphocytic leukemia and small lymphocytic lymphoma by absolute lymphocyte counts in 2,126 patients: 20 years of experience at the University of Texas M.D. Anderson Cancer Center.\" (Tsimberidou AM, J Clin Oncol. 2007 Oct 10;25(29):4648-56.)\\nRetrospective. 2126 consecutive CLL/SLL patients\\nOutcome: rates of response, OS, and FFS not different among different groups\\nPredictive factors: deletion of 17p or 6q, age >60, b2-microglobulin >2, albumin <3.5, creatinine >1.6\\nConclusion: Patients with CLL or SLL can be treated similarly\\n\\n\\nRichter\\'s transformation[edit\\xa0| edit source]\\nNamed for Maurice N. Richter who described it in 1928\\nDevelopment of high grade NHL (typically diffuse large B-cell lymphoma) in the setting of CLL\\nMay be triggered by viral infections (e.g. EBV) or by genetic defects acquired by the malignant clone\\nOccurs in ~4% of CLL patients\\nResponse rates to chemotherapy are low, up to ~40%; median OS is ~8 months\\n\\n\\n1993 PMID 7693038, 1993 — \"Common clonal origin of chronic lymphocytic leukemia and high-grade lymphoma of Richter\\'s syndrome.\" Cherepakhin V et al. Blood. 1993 Nov 15;82(10):3141-7.\\n1975 PMID 1096589, 1975 — \"Richter\\'s syndrome. A terminal complication of chronic lymphocytic leukemia with distinct clinicopathologic features.\" Long JC et al. Am J Clin Pathol. 1975 Jun;63(6):786-95.\\nOriginal description; 1928 PMID 19969796, 1928 — \"Generalized reticular cell sarcoma of lymph nodes associated with lymphatic leukemia.\" Richter MN et al. Am J Pathol. 1928; 4:285.\\nRadiation Therapy[edit\\xa0| edit source]\\nPlease see spleen irradiation\\nReviews[edit\\xa0| edit source]\\n2006 PMID 16983131 -- \"Narrative review: initial management of newly diagnosed, early-stage chronic lymphocytic leukemia.\" (Shanafelt TD, Ann Intern Med. 2006 Sep 19;145(6):435-47.)\\n2006 PMID 16901035 -- \"Chronic lymphocytic leukemia: diagnosis and treatment.\" (Yee KW, Mayo Clin Proc. 2006 Aug;81(8):1105-29.)',)\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "\n",
        "con = sqlite3.connect(\"wikibooks.sqlite\")\n",
        "cur = con.cursor()\n",
        "res = cur.execute(\"SELECT body_text FROM en\")\n",
        "print(res.fetchone())\n",
        "data = res.fetchall()\n",
        "text = \"\"\n",
        "for d in data[:2000]:\n",
        "    text += d[0]\n",
        "corpus = [t[0] for t in data]\n",
        "del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA3BB80XJgPz"
      },
      "source": [
        "# Обучение модели с символьной токенизацией"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text[:2000]  # берём 2000 символов\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "input_data = np.array(sequences[:-1])\n",
        "target_data = np.array(sequences[1:])"
      ],
      "metadata": {
        "id": "_XHf22K6pXB8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf1OiiSl6yOl",
        "outputId": "b1e933c3-5b52-4a69-9f20-61d1a27afbd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f912d1fc190>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "char_model = tf.keras.models.Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=10, input_length=input_data.shape[1]),\n",
        "    SimpleRNN(units=32, return_sequences=True),\n",
        "    TimeDistributed(Dense(units=len(tokenizer.word_index)+1, activation='softmax'))\n",
        "])\n",
        "\n",
        "char_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "char_model.fit(input_data, target_data, epochs=10, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCyXIO98HiR4",
        "outputId": "c4540d62-486d-4293-9ddd-bc4fe921c780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "he te \n"
          ]
        }
      ],
      "source": [
        "generated_text = 'h'\n",
        "for i in range(5):\n",
        "    sequence = tokenizer.texts_to_sequences(generated_text)[-input_data.shape[1]:]\n",
        "    sequence = np.pad(sequence, (0, input_data.shape[1]-len(sequence)), 'constant', constant_values=0)\n",
        "    prediction = char_model.predict(np.array([sequence]))\n",
        "    next_token = np.argmax(prediction[0][-1])\n",
        "    generated_text += tokenizer.index_word[next_token]\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuXu8xH3KZIk"
      },
      "source": [
        "# Обучение модели с пословной токенизацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5FdPiyyM1rzA"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "raw_corpus = corpus[:5]  # берём 5 текстов\n",
        "corpus = []\n",
        "for t in raw_corpus:\n",
        "  corpus.append(re.sub(r'[^a-zA-Z0-9 ]', ' ', t))\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "q7nm-hBh1WNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d71c7c6c-8f9a-4755-ccd2-008cef286bd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "35/35 [==============================] - 31s 834ms/step - loss: 5.9024\n",
            "Epoch 2/10\n",
            "35/35 [==============================] - 28s 804ms/step - loss: 5.4939\n",
            "Epoch 3/10\n",
            "35/35 [==============================] - 26s 732ms/step - loss: 5.4245\n",
            "Epoch 4/10\n",
            "35/35 [==============================] - 26s 740ms/step - loss: 5.4074\n",
            "Epoch 5/10\n",
            "35/35 [==============================] - 27s 744ms/step - loss: 5.3914\n",
            "Epoch 6/10\n",
            "35/35 [==============================] - 25s 721ms/step - loss: 5.3798\n",
            "Epoch 7/10\n",
            "35/35 [==============================] - 24s 682ms/step - loss: 5.3472\n",
            "Epoch 8/10\n",
            "35/35 [==============================] - 24s 683ms/step - loss: 5.3290\n",
            "Epoch 9/10\n",
            "35/35 [==============================] - 24s 670ms/step - loss: 5.1610\n",
            "Epoch 10/10\n",
            "35/35 [==============================] - 23s 651ms/step - loss: 5.0242\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efb88fa34c0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "word_model = tf.keras.Sequential()\n",
        "word_model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "word_model.add(SimpleRNN(100))\n",
        "word_model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "word_model.fit(predictors, tf.keras.utils.to_categorical(label, num_classes=total_words), epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"The professor wants to\"\n",
        "next_words = 1\n",
        "\n",
        "for _ in range(next_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_sequence_len-1, padding='pre')\n",
        "    prob_distribution = word_model.predict(encoded)[0]\n",
        "    prediction = np.argmax(prob_distribution)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == prediction:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAmKE4K6drEg",
        "outputId": "d97f5b74-9d7a-4515-fc5e-bacdaa97a9f2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 105ms/step\n",
            "The professor wants to edit\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}